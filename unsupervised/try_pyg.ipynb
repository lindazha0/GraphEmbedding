{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: eliminating warnings\n",
    "def warn(*args, **kwargs):\n",
    "    pass\n",
    "import warnings\n",
    "warnings.warn = warn\n",
    "\n",
    "from arguments import arg_parse\n",
    "from cortex_DIM.nn_modules.mi_networks import MIFCNet, MI1x1ConvNet\n",
    "from evaluate_embedding import evaluate_embedding\n",
    "from gin import Encoder\n",
    "from losses import local_global_loss_\n",
    "from model import FF, PriorDiscriminator\n",
    "from torch import optim\n",
    "from torch.autograd import Variable\n",
    "from torch_geometric.data import DataLoader\n",
    "from torch_geometric.datasets import TUDataset\n",
    "import json\n",
    "import json\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class arg_parse:\n",
    "    def __init__(self) -> None:\n",
    "        self.DS = 'KKI'\n",
    "        self.local = False\n",
    "        self.glob = False\n",
    "        self.prior = False\n",
    "        self.lr = 0.001\n",
    "        self.num_gc_layers = 2\n",
    "        self.hidden_dim = 32\n",
    "\n",
    "args = arg_parse()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InfoGraph(nn.Module):\n",
    "  def __init__(self, hidden_dim, num_gc_layers, alpha=0.5, beta=1., gamma=.1):\n",
    "    super(InfoGraph, self).__init__()\n",
    "\n",
    "    self.alpha = alpha\n",
    "    self.beta = beta\n",
    "    self.gamma = gamma\n",
    "    self.prior = args.prior\n",
    "\n",
    "    self.embedding_dim = mi_units = hidden_dim * num_gc_layers\n",
    "    self.encoder = Encoder(dataset_num_features, hidden_dim, num_gc_layers)\n",
    "\n",
    "    self.local_d = FF(self.embedding_dim)\n",
    "    self.global_d = FF(self.embedding_dim)\n",
    "    # self.local_d = MI1x1ConvNet(self.embedding_dim, mi_units)\n",
    "    # self.global_d = MIFCNet(self.embedding_dim, mi_units)\n",
    "\n",
    "    if self.prior:\n",
    "        self.prior_d = PriorDiscriminator(self.embedding_dim)\n",
    "\n",
    "    self.init_emb()\n",
    "\n",
    "  def init_emb(self):\n",
    "    initrange = -1.5 / self.embedding_dim\n",
    "    for m in self.modules():\n",
    "        if isinstance(m, nn.Linear):\n",
    "            torch.nn.init.xavier_uniform_(m.weight.data)\n",
    "            if m.bias is not None:\n",
    "                m.bias.data.fill_(0.0)\n",
    "\n",
    "  def forward(self, x, edge_index, batch, num_graphs):\n",
    "    # batch_size = data.num_graphs\n",
    "    if x is None:\n",
    "        x = torch.ones(batch.shape[0]).to(device)\n",
    "\n",
    "    y, M = self.encoder(x, edge_index, batch)\n",
    "\n",
    "    g_enc = self.global_d(y)\n",
    "    l_enc = self.local_d(M)\n",
    "\n",
    "    mode='fd'\n",
    "    measure='JSD'\n",
    "    local_global_loss = local_global_loss_(l_enc, g_enc, edge_index, batch, measure)\n",
    "\n",
    "    if self.prior:\n",
    "        prior = torch.rand_like(y)\n",
    "        term_a = torch.log(self.prior_d(prior)).mean()\n",
    "        term_b = torch.log(1.0 - self.prior_d(y)).mean()\n",
    "        PRIOR = - (term_a + term_b) * self.gamma\n",
    "    else:\n",
    "        PRIOR = 0\n",
    "\n",
    "    return local_global_loss + PRIOR\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.data import InMemoryDataset\n",
    "\n",
    "class KKIData(InMemoryDataset):\n",
    "    def __init__(self, root='../data/KKI/KKI/', transform= None, pre_transform=None, pre_filter = None):\n",
    "        super().__init__(root, transform, pre_transform, pre_filter)\n",
    "        out = torch.load(self.processed_paths[0])\n",
    "        self.data, self.slices, self.sizes = out\n",
    "\n",
    "    @property\n",
    "    def raw_file_names(self):\n",
    "        return ['KKI_A.txt']\n",
    "\n",
    "    @property\n",
    "    def processed_file_names(self):\n",
    "        return ['data.pt']\n",
    "\n",
    "dataset = KKIData()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'edge_index': tensor([   0,  406,  514,  558,  598,  638,  694,  836, 1062, 1254, 1374, 1408,\n",
       "         1622, 1836, 1940, 2012, 2038, 2082, 2356, 2628, 3102, 3142, 3174, 3232,\n",
       "         3290, 3308, 3348, 3368, 3398, 3428, 3462, 3688, 3708, 3774, 3858, 3952,\n",
       "         4010, 4032, 4056, 4080, 4096, 4148, 4216, 4236, 4436, 4570, 4820, 4892,\n",
       "         4936, 4972, 4998, 5010, 5174, 5236, 5278, 5312, 5490, 5684, 5742, 5816,\n",
       "         5852, 6014, 6046, 6088, 6218, 6502, 6580, 6626, 6712, 6834, 6872, 6980,\n",
       "         7088, 7110, 7202, 7300, 7358, 7366, 7520, 7548, 7920, 7948, 8022, 8038]),\n",
       " 'x': tensor([   0,   77,  112,  125,  138,  153,  174,  223,  285,  337,  371,  384,\n",
       "          442,  507,  542,  570,  582,  599,  657,  729,  795,  812,  824,  839,\n",
       "          857,  866,  882,  892,  904,  918,  932,  978,  987, 1015, 1031, 1060,\n",
       "         1080, 1088, 1100, 1111, 1119, 1137, 1160, 1170, 1214, 1258, 1310, 1321,\n",
       "         1336, 1350, 1362, 1369, 1416, 1430, 1441, 1455, 1494, 1552, 1572, 1590,\n",
       "         1607, 1652, 1666, 1681, 1712, 1783, 1808, 1826, 1854, 1892, 1908, 1938,\n",
       "         1965, 1973, 2007, 2035, 2055, 2060, 2102, 2115, 2205, 2215, 2232, 2238]),\n",
       " 'y': tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n",
       "         18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,\n",
       "         36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,\n",
       "         54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71,\n",
       "         72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83])}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.slices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# accuracies = {'logreg': [], 'svc': [], 'linearsvc': [], 'randomforest': []}\n",
    "epochs = 2\n",
    "log_interval = 1\n",
    "batch_size = 128\n",
    "lr = args.lr\n",
    "DS = args.DS\n",
    "path = os.path.join(os.getcwd(), '..', 'data', DS)\n",
    "# kf = StratifiedKFold(n_splits=10, shuffle=True, random_state=None)\n",
    "\n",
    "# datapath = path+'/KKI/processed/data.pt'\n",
    "# dataset = torch.load(datapath)[0]\n",
    "# dataset = TUDataset(path, name=DS).shuffle()\n",
    "dataset_num_features = max(dataset.num_features, 1)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.int64"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.data.y.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================\n",
      "lr: 0.001\n",
      "num_features: 190\n",
      "hidden_dim: 32\n",
      "num_gc_layers: 2\n",
      "================\n",
      "===== Before training =====\n",
      "embedding length:\n",
      "[[3.17232490e+00 9.59328473e-01 1.80678892e+00 ... 3.30266058e-01\n",
      "  5.39329338e+00 1.51378889e+01]\n",
      " [2.03503036e+00 7.26320297e-02 2.29423451e+00 ... 2.40086228e-01\n",
      "  1.54712951e+00 5.59823322e+00]\n",
      " [1.56246972e+00 0.00000000e+00 4.64691455e-03 ... 0.00000000e+00\n",
      "  2.67243886e+00 5.49802876e+00]\n",
      " ...\n",
      " [3.01488191e-01 7.46013736e-03 1.05110705e-02 ... 0.00000000e+00\n",
      "  2.12603331e+00 2.15811729e-01]\n",
      " [1.23019493e+00 3.08957040e-01 6.29850149e-01 ... 0.00000000e+00\n",
      "  7.20523894e-01 6.69880137e-02]\n",
      " [3.18962075e-02 5.05512990e-02 4.75434586e-02 ... 0.00000000e+00\n",
      "  2.47058123e-01 0.00000000e+00]]\n"
     ]
    }
   ],
   "source": [
    "print('================')\n",
    "print('lr: {}'.format(lr))\n",
    "print('num_features: {}'.format(dataset_num_features))\n",
    "print('hidden_dim: {}'.format(args.hidden_dim))\n",
    "print('num_gc_layers: {}'.format(args.num_gc_layers))\n",
    "print('================')\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = InfoGraph(args.hidden_dim, args.num_gc_layers).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "model.eval()\n",
    "emb = model.encoder.get_embeddings(dataloader)\n",
    "print('===== Before training =====')\n",
    "# res = evaluate_embedding(emb)\n",
    "# accuracies['logreg'].append(res[0])\n",
    "# accuracies['svc'].append(res[1])\n",
    "# accuracies['linearsvc'].append(res[2])\n",
    "# accuracies['randomforest'].append(res[3])\n",
    "# print(accuracies)\n",
    "print(f\"embedding length:\\n{emb}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== Epoch 1, Loss 6082.568550109863 =====\n",
      "embedding shape:\n",
      "83x64\n",
      "===== Epoch 2, Loss 4675.409149169922 =====\n",
      "embedding shape:\n",
      "83x64\n"
     ]
    }
   ],
   "source": [
    "# unsupervised train\n",
    "for epoch in range(1, epochs+1):\n",
    "    loss_all = 0\n",
    "    model.train()\n",
    "    for data in dataloader:\n",
    "        data = data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        loss = model(data.x, data.edge_index, data.batch, data.num_graphs)\n",
    "        loss_all += loss.item() * data.num_graphs\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print('===== Epoch {}, Loss {} ====='.format(\n",
    "        epoch, loss_all / len(dataloader)))\n",
    "\n",
    "    if epoch % log_interval == 0:\n",
    "        model.eval()\n",
    "        emb = model.encoder.get_embeddings(dataloader)\n",
    "        print(f\"embedding shape:\\n{len(emb)}x{len(emb[0])}\")\n",
    "\n",
    "        # res = evaluate_embedding(emb, y)\n",
    "        # accuracies['logreg'].append(res[0])\n",
    "        # accuracies['svc'].append(res[1])\n",
    "        # accuracies['linearsvc'].append(res[2])\n",
    "        # accuracies['randomforest'].append(res[3])\n",
    "        # print(accuracies)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs150",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
