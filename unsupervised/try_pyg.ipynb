{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: eliminating warnings\n",
    "def warn(*args, **kwargs):\n",
    "    pass\n",
    "import warnings\n",
    "warnings.warn = warn\n",
    "\n",
    "from arguments import arg_parse\n",
    "from cortex_DIM.nn_modules.mi_networks import MIFCNet, MI1x1ConvNet\n",
    "from evaluate_embedding import evaluate_embedding\n",
    "from gin import Encoder\n",
    "from losses import local_global_loss_\n",
    "from model import FF, PriorDiscriminator\n",
    "from torch import optim\n",
    "from torch.autograd import Variable\n",
    "from torch_geometric.data import DataLoader\n",
    "from torch_geometric.datasets import TUDataset\n",
    "import json\n",
    "import json\n",
    "import numpy as np\n",
    "import os.path as osp\n",
    "import sys\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class InfoGraph(nn.Module):\n",
    "  def __init__(self, hidden_dim, num_gc_layers, alpha=0.5, beta=1., gamma=.1):\n",
    "    super(InfoGraph, self).__init__()\n",
    "\n",
    "    self.alpha = alpha\n",
    "    self.beta = beta\n",
    "    self.gamma = gamma\n",
    "    self.prior = args.prior\n",
    "\n",
    "    self.embedding_dim = mi_units = hidden_dim * num_gc_layers\n",
    "    self.encoder = Encoder(dataset_num_features, hidden_dim, num_gc_layers)\n",
    "\n",
    "    self.local_d = FF(self.embedding_dim)\n",
    "    self.global_d = FF(self.embedding_dim)\n",
    "    # self.local_d = MI1x1ConvNet(self.embedding_dim, mi_units)\n",
    "    # self.global_d = MIFCNet(self.embedding_dim, mi_units)\n",
    "\n",
    "    if self.prior:\n",
    "        self.prior_d = PriorDiscriminator(self.embedding_dim)\n",
    "\n",
    "    self.init_emb()\n",
    "\n",
    "  def init_emb(self):\n",
    "    initrange = -1.5 / self.embedding_dim\n",
    "    for m in self.modules():\n",
    "        if isinstance(m, nn.Linear):\n",
    "            torch.nn.init.xavier_uniform_(m.weight.data)\n",
    "            if m.bias is not None:\n",
    "                m.bias.data.fill_(0.0)\n",
    "\n",
    "  def forward(self, x, edge_index, batch, num_graphs):\n",
    "    # batch_size = data.num_graphs\n",
    "    if x is None:\n",
    "        x = torch.ones(batch.shape[0]).to(device)\n",
    "\n",
    "    y, M = self.encoder(x, edge_index, batch)\n",
    "\n",
    "    g_enc = self.global_d(y)\n",
    "    l_enc = self.local_d(M)\n",
    "\n",
    "    mode='fd'\n",
    "    measure='JSD'\n",
    "    local_global_loss = local_global_loss_(l_enc, g_enc, edge_index, batch, measure)\n",
    "\n",
    "    if self.prior:\n",
    "        prior = torch.rand_like(y)\n",
    "        term_a = torch.log(self.prior_d(prior)).mean()\n",
    "        term_b = torch.log(1.0 - self.prior_d(y)).mean()\n",
    "        PRIOR = - (term_a + term_b) * self.gamma\n",
    "    else:\n",
    "        PRIOR = 0\n",
    "\n",
    "    return local_global_loss + PRIOR\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name '__file__' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 15\u001b[0m\n\u001b[1;32m     13\u001b[0m lr \u001b[39m=\u001b[39m args\u001b[39m.\u001b[39mlr\n\u001b[1;32m     14\u001b[0m DS \u001b[39m=\u001b[39m args\u001b[39m.\u001b[39mDS\n\u001b[0;32m---> 15\u001b[0m path \u001b[39m=\u001b[39m osp\u001b[39m.\u001b[39mjoin(osp\u001b[39m.\u001b[39mdirname(osp\u001b[39m.\u001b[39mrealpath(\u001b[39m__file__\u001b[39;49m)), \u001b[39m'\u001b[39m\u001b[39m..\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mdata\u001b[39m\u001b[39m'\u001b[39m, DS)\n\u001b[1;32m     16\u001b[0m \u001b[39m# kf = StratifiedKFold(n_splits=10, shuffle=True, random_state=None)\u001b[39;00m\n\u001b[1;32m     18\u001b[0m dataset \u001b[39m=\u001b[39m TUDataset(path, name\u001b[39m=\u001b[39mDS)\u001b[39m.\u001b[39mshuffle()\n",
      "\u001b[0;31mNameError\u001b[0m: name '__file__' is not defined"
     ]
    }
   ],
   "source": [
    "class arg_parse:\n",
    "    def __init__(self) -> None:\n",
    "        self.lr = 0.001\n",
    "        self.DS = 'KKI'\n",
    "        self.num_gc_layers = 2\n",
    "        self.hidden_dim = 32\n",
    "\n",
    "args = arg_parse()\n",
    "accuracies = {'logreg': [], 'svc': [], 'linearsvc': [], 'randomforest': []}\n",
    "epochs = 2\n",
    "log_interval = 1\n",
    "batch_size = 128\n",
    "lr = args.lr\n",
    "DS = args.DS\n",
    "path = osp.join(osp.dirname(osp.realpath(__file__)), '..', 'data', DS)\n",
    "# kf = StratifiedKFold(n_splits=10, shuffle=True, random_state=None)\n",
    "\n",
    "dataset = TUDataset(path, name=DS).shuffle()\n",
    "dataset_num_features = max(dataset.num_features, 1)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size)\n",
    "\n",
    "print('================')\n",
    "print('lr: {}'.format(lr))\n",
    "print('num_features: {}'.format(dataset_num_features))\n",
    "print('hidden_dim: {}'.format(args.hidden_dim))\n",
    "print('num_gc_layers: {}'.format(args.num_gc_layers))\n",
    "print('================')\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = InfoGraph(args.hidden_dim, args.num_gc_layers).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "model.eval()\n",
    "emb, y = model.encoder.get_embeddings(dataloader)\n",
    "print('===== Before training =====')\n",
    "res = evaluate_embedding(emb, y)\n",
    "accuracies['logreg'].append(res[0])\n",
    "accuracies['svc'].append(res[1])\n",
    "accuracies['linearsvc'].append(res[2])\n",
    "accuracies['randomforest'].append(res[3])\n",
    "print(accuracies)\n",
    "print(f\"embedding length:\\n{emb}, labels:\\n{y}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unsupervised train\n",
    "for epoch in range(1, epochs+1):\n",
    "    loss_all = 0\n",
    "    model.train()\n",
    "    for data in dataloader:\n",
    "        data = data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        loss = model(data.x, data.edge_index, data.batch, data.num_graphs)\n",
    "        loss_all += loss.item() * data.num_graphs\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print('===== Epoch {}, Loss {} ====='.format(\n",
    "        epoch, loss_all / len(dataloader)))\n",
    "\n",
    "    if epoch % log_interval == 0:\n",
    "        model.eval()\n",
    "        emb, y = model.encoder.get_embeddings(dataloader)\n",
    "        res = evaluate_embedding(emb, y)\n",
    "        accuracies['logreg'].append(res[0])\n",
    "        accuracies['svc'].append(res[1])\n",
    "        accuracies['linearsvc'].append(res[2])\n",
    "        accuracies['randomforest'].append(res[3])\n",
    "        print(accuracies)\n",
    "\n",
    "with open('unsupervised.log', 'a+') as f:\n",
    "    s = json.dumps(accuracies)\n",
    "    f.write('{},{},{},{},{},{}\\n'.format(\n",
    "        args.DS, args.num_gc_layers, epochs, log_interval, lr, s))\n",
    "    f.write('embedding: \\n{}'.format(emb))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs150",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
