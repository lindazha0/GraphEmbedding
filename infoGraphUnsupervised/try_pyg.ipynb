{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: eliminating warnings\n",
    "def warn(*args, **kwargs):\n",
    "    pass\n",
    "import warnings\n",
    "warnings.warn = warn\n",
    "\n",
    "from arguments import arg_parse\n",
    "from cortex_DIM.nn_modules.mi_networks import MIFCNet, MI1x1ConvNet\n",
    "from evaluate_embedding import evaluate_embedding\n",
    "from gin import Encoder\n",
    "from losses import local_global_loss_\n",
    "from model import FF, PriorDiscriminator\n",
    "from torch import optim\n",
    "from torch.autograd import Variable\n",
    "from torch_geometric.data import DataLoader\n",
    "from torch_geometric.datasets import TUDataset\n",
    "import json\n",
    "import json\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class arg_parse:\n",
    "    def __init__(self) -> None:\n",
    "        self.DS = 'KKI'\n",
    "        self.local = False\n",
    "        self.glob = False\n",
    "        self.prior = False\n",
    "        self.lr = 0.001\n",
    "        self.num_gc_layers = 2\n",
    "        self.hidden_dim = 32\n",
    "\n",
    "args = arg_parse()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InfoGraph(nn.Module):\n",
    "  def __init__(self, hidden_dim, num_gc_layers, alpha=0.5, beta=1., gamma=.1):\n",
    "    super(InfoGraph, self).__init__()\n",
    "\n",
    "    self.alpha = alpha\n",
    "    self.beta = beta\n",
    "    self.gamma = gamma\n",
    "    self.prior = args.prior\n",
    "\n",
    "    self.embedding_dim = mi_units = hidden_dim * num_gc_layers\n",
    "    self.encoder = Encoder(dataset_num_features, hidden_dim, num_gc_layers)\n",
    "\n",
    "    self.local_d = FF(self.embedding_dim)\n",
    "    self.global_d = FF(self.embedding_dim)\n",
    "    # self.local_d = MI1x1ConvNet(self.embedding_dim, mi_units)\n",
    "    # self.global_d = MIFCNet(self.embedding_dim, mi_units)\n",
    "\n",
    "    if self.prior:\n",
    "        self.prior_d = PriorDiscriminator(self.embedding_dim)\n",
    "\n",
    "    self.init_emb()\n",
    "\n",
    "  def init_emb(self):\n",
    "    initrange = -1.5 / self.embedding_dim\n",
    "    for m in self.modules():\n",
    "        if isinstance(m, nn.Linear):\n",
    "            torch.nn.init.xavier_uniform_(m.weight.data)\n",
    "            if m.bias is not None:\n",
    "                m.bias.data.fill_(0.0)\n",
    "\n",
    "  def forward(self, x, edge_index, batch, num_graphs):\n",
    "    # batch_size = data.num_graphs\n",
    "    if x is None:\n",
    "        x = torch.ones(batch.shape[0]).to(device)\n",
    "\n",
    "    y, M = self.encoder(x, edge_index, batch)\n",
    "\n",
    "    g_enc = self.global_d(y)\n",
    "    l_enc = self.local_d(M)\n",
    "\n",
    "    mode='fd'\n",
    "    measure='JSD'\n",
    "    local_global_loss = local_global_loss_(l_enc, g_enc, edge_index, batch, measure)\n",
    "\n",
    "    if self.prior:\n",
    "        prior = torch.rand_like(y)\n",
    "        term_a = torch.log(self.prior_d(prior)).mean()\n",
    "        term_b = torch.log(1.0 - self.prior_d(y)).mean()\n",
    "        PRIOR = - (term_a + term_b) * self.gamma\n",
    "    else:\n",
    "        PRIOR = 0\n",
    "\n",
    "    return local_global_loss + PRIOR\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.data import InMemoryDataset\n",
    "\n",
    "class KKIData(InMemoryDataset):\n",
    "    def __init__(self, root='../data/KKI/KKI/', transform= None, pre_transform=None, pre_filter = None):\n",
    "        super().__init__(root, transform, pre_transform, pre_filter)\n",
    "        out = torch.load(self.processed_paths[0])\n",
    "        self.data, self.slices, self.sizes = out\n",
    "\n",
    "    @property\n",
    "    def raw_file_names(self):\n",
    "        return ['KKI_A.txt']\n",
    "\n",
    "    @property\n",
    "    def processed_file_names(self):\n",
    "        return ['data.pt']\n",
    "\n",
    "dataset = KKIData()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch_geometric.data import Data, InMemoryDataset\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "class MyData(InMemoryDataset):\n",
    "    def __init__(self, root='../mydata/', transform=None, pre_transform=None, pre_filter=None):\n",
    "        super().__init__(root, transform, pre_transform, pre_filter)\n",
    "        x = torch.tensor(\n",
    "            [[0], [0], [0], [1], [1], [1], [1]], dtype=torch.float32\n",
    "        )\n",
    "        edge_index = torch.tensor([\n",
    "            [0, 1, 0, 3, 4, 5],\n",
    "            [1, 2, 2, 4, 5, 6]\n",
    "        ], dtype=torch.int64)\n",
    "        # y = torch.tensor([1,1], dtype=torch.int64)\n",
    "        self.data = Data(x=x, edge_index=edge_index)\n",
    "        self.slices = {\n",
    "            'x': x, 'edge_index': edge_index\n",
    "        }\n",
    "mydata = MyData()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "only one element tensors can be converted to Python scalars",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[76], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m myloader \u001b[39m=\u001b[39m DataLoader(mydata, batch_size\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m)\n\u001b[0;32m----> 2\u001b[0m \u001b[39mfor\u001b[39;00m data \u001b[39min\u001b[39;00m mydata:\n\u001b[1;32m      3\u001b[0m     \u001b[39mprint\u001b[39m(data)\n",
      "File \u001b[0;32m/Applications/anaconda3/envs/cs150/lib/python3.8/site-packages/torch_geometric/data/dataset.py:239\u001b[0m, in \u001b[0;36mDataset.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m    229\u001b[0m \u001b[39m\u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\"In case :obj:`idx` is of type integer, will return the data object\u001b[39;00m\n\u001b[1;32m    230\u001b[0m \u001b[39mat index :obj:`idx` (and transforms it in case :obj:`transform` is\u001b[39;00m\n\u001b[1;32m    231\u001b[0m \u001b[39mpresent).\u001b[39;00m\n\u001b[1;32m    232\u001b[0m \u001b[39mIn case :obj:`idx` is a slicing object, *e.g.*, :obj:`[2:5]`, a list, a\u001b[39;00m\n\u001b[1;32m    233\u001b[0m \u001b[39mtuple, or a :obj:`torch.Tensor` or :obj:`np.ndarray` of type long or\u001b[39;00m\n\u001b[1;32m    234\u001b[0m \u001b[39mbool, will return a subset of the dataset at the specified indices.\"\"\"\u001b[39;00m\n\u001b[1;32m    235\u001b[0m \u001b[39mif\u001b[39;00m (\u001b[39misinstance\u001b[39m(idx, (\u001b[39mint\u001b[39m, np\u001b[39m.\u001b[39minteger))\n\u001b[1;32m    236\u001b[0m         \u001b[39mor\u001b[39;00m (\u001b[39misinstance\u001b[39m(idx, Tensor) \u001b[39mand\u001b[39;00m idx\u001b[39m.\u001b[39mdim() \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m)\n\u001b[1;32m    237\u001b[0m         \u001b[39mor\u001b[39;00m (\u001b[39misinstance\u001b[39m(idx, np\u001b[39m.\u001b[39mndarray) \u001b[39mand\u001b[39;00m np\u001b[39m.\u001b[39misscalar(idx))):\n\u001b[0;32m--> 239\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mget(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mindices()[idx])\n\u001b[1;32m    240\u001b[0m     data \u001b[39m=\u001b[39m data \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtransform \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtransform(data)\n\u001b[1;32m    241\u001b[0m     \u001b[39mreturn\u001b[39;00m data\n",
      "File \u001b[0;32m/Applications/anaconda3/envs/cs150/lib/python3.8/site-packages/torch_geometric/data/in_memory_dataset.py:82\u001b[0m, in \u001b[0;36mInMemoryDataset.get\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_data_list[idx] \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m     80\u001b[0m     \u001b[39mreturn\u001b[39;00m copy\u001b[39m.\u001b[39mcopy(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_data_list[idx])\n\u001b[0;32m---> 82\u001b[0m data \u001b[39m=\u001b[39m separate(\n\u001b[1;32m     83\u001b[0m     \u001b[39mcls\u001b[39;49m\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdata\u001b[39m.\u001b[39;49m\u001b[39m__class__\u001b[39;49m,\n\u001b[1;32m     84\u001b[0m     batch\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdata,\n\u001b[1;32m     85\u001b[0m     idx\u001b[39m=\u001b[39;49midx,\n\u001b[1;32m     86\u001b[0m     slice_dict\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mslices,\n\u001b[1;32m     87\u001b[0m     decrement\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m     88\u001b[0m )\n\u001b[1;32m     90\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_data_list[idx] \u001b[39m=\u001b[39m copy\u001b[39m.\u001b[39mcopy(data)\n\u001b[1;32m     92\u001b[0m \u001b[39mreturn\u001b[39;00m data\n",
      "File \u001b[0;32m/Applications/anaconda3/envs/cs150/lib/python3.8/site-packages/torch_geometric/data/separate.py:37\u001b[0m, in \u001b[0;36mseparate\u001b[0;34m(cls, batch, idx, slice_dict, inc_dict, decrement)\u001b[0m\n\u001b[1;32m     35\u001b[0m         slices \u001b[39m=\u001b[39m slice_dict[attr]\n\u001b[1;32m     36\u001b[0m         incs \u001b[39m=\u001b[39m inc_dict[attr] \u001b[39mif\u001b[39;00m decrement \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m---> 37\u001b[0m     data_store[attr] \u001b[39m=\u001b[39m _separate(attr, batch_store[attr], idx, slices,\n\u001b[1;32m     38\u001b[0m                                  incs, batch, batch_store, decrement)\n\u001b[1;32m     40\u001b[0m \u001b[39m# The `num_nodes` attribute needs special treatment, as we cannot infer\u001b[39;00m\n\u001b[1;32m     41\u001b[0m \u001b[39m# the real number of nodes from the total number of nodes alone:\u001b[39;00m\n\u001b[1;32m     42\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(batch_store, \u001b[39m'\u001b[39m\u001b[39m_num_nodes\u001b[39m\u001b[39m'\u001b[39m):\n",
      "File \u001b[0;32m/Applications/anaconda3/envs/cs150/lib/python3.8/site-packages/torch_geometric/data/separate.py:64\u001b[0m, in \u001b[0;36m_separate\u001b[0;34m(key, value, idx, slices, incs, batch, store, decrement)\u001b[0m\n\u001b[1;32m     62\u001b[0m key \u001b[39m=\u001b[39m \u001b[39mstr\u001b[39m(key)\n\u001b[1;32m     63\u001b[0m cat_dim \u001b[39m=\u001b[39m batch\u001b[39m.\u001b[39m__cat_dim__(key, value, store)\n\u001b[0;32m---> 64\u001b[0m start, end \u001b[39m=\u001b[39m \u001b[39mint\u001b[39;49m(slices[idx]), \u001b[39mint\u001b[39m(slices[idx \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m])\n\u001b[1;32m     65\u001b[0m value \u001b[39m=\u001b[39m value\u001b[39m.\u001b[39mnarrow(cat_dim \u001b[39mor\u001b[39;00m \u001b[39m0\u001b[39m, start, end \u001b[39m-\u001b[39m start)\n\u001b[1;32m     66\u001b[0m value \u001b[39m=\u001b[39m value\u001b[39m.\u001b[39msqueeze(\u001b[39m0\u001b[39m) \u001b[39mif\u001b[39;00m cat_dim \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m value\n",
      "\u001b[0;31mValueError\u001b[0m: only one element tensors can be converted to Python scalars"
     ]
    }
   ],
   "source": [
    "myloader = DataLoader(mydata, batch_size=2)\n",
    "for data in mydata:\n",
    "    print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataBatch(edge_index=[2, 8038], x=[2238, 190], y=[83], batch=[2238], ptr=[84])\n"
     ]
    }
   ],
   "source": [
    "dataloader =  DataLoader(dataset, batch_size=128)\n",
    "for data in dataloader:\n",
    "    print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# accuracies = {'logreg': [], 'svc': [], 'linearsvc': [], 'randomforest': []}\n",
    "epochs = 2\n",
    "log_interval = 1\n",
    "batch_size = 128\n",
    "lr = args.lr\n",
    "DS = args.DS\n",
    "path = os.path.join(os.getcwd(), '..', 'data', DS)\n",
    "# kf = StratifiedKFold(n_splits=10, shuffle=True, random_state=None)\n",
    "\n",
    "# datapath = path+'/KKI/processed/data.pt'\n",
    "# dataset = torch.load(datapath)[0]\n",
    "# dataset = TUDataset(path, name=DS).shuffle()\n",
    "dataset_num_features = max(dataset.num_features, 1)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================\n",
      "lr: 0.001\n",
      "num_features: 190\n",
      "hidden_dim: 32\n",
      "num_gc_layers: 2\n",
      "================\n",
      "===== Before training =====\n",
      "embedding length:\n",
      "[[3.17232490e+00 9.59328473e-01 1.80678892e+00 ... 3.30266058e-01\n",
      "  5.39329338e+00 1.51378889e+01]\n",
      " [2.03503036e+00 7.26320297e-02 2.29423451e+00 ... 2.40086228e-01\n",
      "  1.54712951e+00 5.59823322e+00]\n",
      " [1.56246972e+00 0.00000000e+00 4.64691455e-03 ... 0.00000000e+00\n",
      "  2.67243886e+00 5.49802876e+00]\n",
      " ...\n",
      " [3.01488191e-01 7.46013736e-03 1.05110705e-02 ... 0.00000000e+00\n",
      "  2.12603331e+00 2.15811729e-01]\n",
      " [1.23019493e+00 3.08957040e-01 6.29850149e-01 ... 0.00000000e+00\n",
      "  7.20523894e-01 6.69880137e-02]\n",
      " [3.18962075e-02 5.05512990e-02 4.75434586e-02 ... 0.00000000e+00\n",
      "  2.47058123e-01 0.00000000e+00]]\n"
     ]
    }
   ],
   "source": [
    "print('================')\n",
    "print('lr: {}'.format(lr))\n",
    "print('num_features: {}'.format(dataset_num_features))\n",
    "print('hidden_dim: {}'.format(args.hidden_dim))\n",
    "print('num_gc_layers: {}'.format(args.num_gc_layers))\n",
    "print('================')\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = InfoGraph(args.hidden_dim, args.num_gc_layers).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "model.eval()\n",
    "emb = model.encoder.get_embeddings(dataloader)\n",
    "print('===== Before training =====')\n",
    "# res = evaluate_embedding(emb)\n",
    "# accuracies['logreg'].append(res[0])\n",
    "# accuracies['svc'].append(res[1])\n",
    "# accuracies['linearsvc'].append(res[2])\n",
    "# accuracies['randomforest'].append(res[3])\n",
    "# print(accuracies)\n",
    "print(f\"embedding length:\\n{emb}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== Epoch 1, Loss 6082.568550109863 =====\n",
      "embedding shape:\n",
      "83x64\n",
      "===== Epoch 2, Loss 4675.409149169922 =====\n",
      "embedding shape:\n",
      "83x64\n"
     ]
    }
   ],
   "source": [
    "# unsupervised train\n",
    "for epoch in range(1, epochs+1):\n",
    "    loss_all = 0\n",
    "    model.train()\n",
    "    for data in dataloader:\n",
    "        data = data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        loss = model(data.x, data.edge_index, data.batch, data.num_graphs)\n",
    "        loss_all += loss.item() * data.num_graphs\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print('===== Epoch {}, Loss {} ====='.format(\n",
    "        epoch, loss_all / len(dataloader)))\n",
    "\n",
    "    if epoch % log_interval == 0:\n",
    "        model.eval()\n",
    "        emb = model.encoder.get_embeddings(dataloader)\n",
    "        print(f\"embedding shape:\\n{len(emb)}x{len(emb[0])}\")\n",
    "\n",
    "        # res = evaluate_embedding(emb, y)\n",
    "        # accuracies['logreg'].append(res[0])\n",
    "        # accuracies['svc'].append(res[1])\n",
    "        # accuracies['linearsvc'].append(res[2])\n",
    "        # accuracies['randomforest'].append(res[3])\n",
    "        # print(accuracies)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs150",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
